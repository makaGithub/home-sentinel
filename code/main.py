#!/usr/bin/env python3
"""
main.py ‚Äî –º–æ–¥—É–ª—å–Ω—ã–π home-sentinel –¥–ª—è Immich.

–§—É–Ω–∫—Ü–∏–∏:
- YOLOv11 –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
- InsightFace (antelopev2) —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑ Immich)
- –ê–Ω—Ç–∏-–¥—Ä–µ–±–µ–∑–≥
- –õ–æ–≥ "–∫—Ç–æ –≤ –∫–∞–¥—Ä–µ"
- –ó–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ª—é–¥—è–º
- –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–≥–æ –∞—É–¥–∏–æ–¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Å RTSP (AudioDetector) –∏ –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∑–≤—É–∫–∞–º (–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ)
"""

# –ü–µ—Ä–≤—ã–º –≥–ª—É—à–∏–º C-level stdout/stderr
import c_silence  # noqa: F401

import os
import time
import warnings
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ CPU –ø–æ—Ç–æ–∫–æ–≤ (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ –∏–º–ø–æ—Ä—Ç–∞ cv2/numpy)
import config
num_threads = config.CPU_THREADS
os.environ["OPENCV_NUM_THREADS"] = str(num_threads)
os.environ["OMP_NUM_THREADS"] = str(num_threads)
os.environ["MKL_NUM_THREADS"] = str(num_threads)
os.environ["NUMEXPR_NUM_THREADS"] = str(num_threads)

import cv2
cv2.setNumThreads(num_threads)
import numpy as np
from PIL import Image, ImageDraw, ImageFont

import stats
# from audio_detector import AudioDetector  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ
from camera import open_camera_stream
from embeddings import load_or_refresh_cache
from models import init_face_analysis, init_yolo
from utils import (
    adaptive_threshold,
    ensure_dirs,
    log,
    preprocess_face_crop,
    _l2_normalize,
)

warnings.filterwarnings("ignore")


# ============================================================
# üé® –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode
# ============================================================
def draw_text_unicode(img, text, position, font_size=20, text_color=(255, 255, 255), bg_color=None):
    """
    –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode (–≤–∫–ª—é—á–∞—è —Ä—É—Å—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã) –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ OpenCV.
    
    Args:
        img: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ OpenCV (BGR)
        text: —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
        position: (x, y) –ø–æ–∑–∏—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        font_size: —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞
        text_color: —Ü–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞ (RGB)
        bg_color: —Ü–≤–µ—Ç —Ñ–æ–Ω–∞ (RGB) –∏–ª–∏ None
    
    Returns:
        –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –æ—Ç—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
    """
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º OpenCV –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (BGR) –≤ PIL (RGB)
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —à—Ä–∏—Ñ—Ç, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º default
    try:
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π —à—Ä–∏—Ñ—Ç
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", font_size)
    except:
        try:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—É—Ç—å
            font = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf", font_size)
        except:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º default —à—Ä–∏—Ñ—Ç (–º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã)
            font = ImageFont.load_default()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–∞
    bbox = draw.textbbox((0, 0), text, font=font)
    text_width = bbox[2] - bbox[0]
    text_height = bbox[3] - bbox[1]
    
    x, y = position
    
    # –†–∏—Å—É–µ–º —Ñ–æ–Ω –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if bg_color:
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –æ—Ç—Å—Ç—É–ø
        padding = 3
        draw.rectangle(
            [(x - padding, y - text_height - padding - 2), (x + text_width + padding, y + padding)],
            fill=bg_color,
            outline=(0, 0, 0),  # –ß—ë—Ä–Ω–∞—è –æ–±–≤–æ–¥–∫–∞ —Ä–∞–º–∫–∏
            width=1
        )
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —Ü–≤–µ—Ç–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è—Ä–∫–æ—Å—Ç–∏ —Ñ–æ–Ω–∞
        # –§–æ—Ä–º—É–ª–∞ —è—Ä–∫–æ—Å—Ç–∏: 0.299*R + 0.587*G + 0.114*B
        brightness = 0.299 * bg_color[0] + 0.587 * bg_color[1] + 0.114 * bg_color[2]
        text_color = (0, 0, 0) if brightness > 128 else (255, 255, 255)
    
    # –†–∏—Å—É–µ–º —Ç–µ–∫—Å—Ç —Å —Ç–µ–Ω—å—é –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    shadow_color = (0, 0, 0) if text_color == (255, 255, 255) else (255, 255, 255)
    draw.text((x + 1, y - text_height + 1), text, font=font, fill=shadow_color)  # –¢–µ–Ω—å
    draw.text((x, y - text_height), text, font=font, fill=text_color)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ OpenCV (BGR)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)


# ============================================================
# üí° –§—É–Ω–∫—Ü–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –ª–∏—Ü–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞
# ============================================================
def compute_face_similarity(
    face_emb: np.ndarray,
    person_embeddings_list: list,
    person_confidences: list | None = None,
) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ª–∏—Ü–æ–º –∏ –≤—Å–µ–º–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞.
    –ù–µ–º–Ω–æ–≥–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç confidence, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å.
    """
    if not person_embeddings_list:
        return 0.0

    face_emb = _l2_normalize(face_emb)
    max_sim = -1.0

    for idx, emb in enumerate(person_embeddings_list):
        emb = _l2_normalize(emb)
        sim = float(np.dot(face_emb, emb))

        if person_confidences and idx < len(person_confidences):
            conf = max(0.5, float(person_confidences[idx]))
            sim *= (0.7 + 0.3 * conf)

        if sim > max_sim:
            max_sim = sim

    return max_sim


# ============================================================
# üîÅ –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª: –≤–∏–¥–µ–æ, –æ–±—ä–µ–∫—Ç—ã, –ª–∏—Ü–∞, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
# ============================================================
def recognize_objects_and_faces(
    all_embeddings_list: list,
    names: list,
    all_confidences_list: list,
):
    ensure_dirs()

    log(f"‚öôÔ∏è  CPU threads: {config.CPU_THREADS} (OpenCV: {cv2.getNumThreads()})")

    stream = open_camera_stream()
    if stream is None:
        time.sleep(5)
        stream = open_camera_stream()
        if stream is None:
            raise RuntimeError("üö´ –ö–∞–º–µ—Ä–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

    yolo = init_yolo()
    face_app = init_face_analysis()

    tracked: dict[str, dict[str, int]] = {}
    last_reported: set[str] = set()
    frame = 0

    face_cache: dict[int, tuple[int, str | None, float]] = {}
    cache_validity = config.FACE_CACHE_VALIDITY_FRAMES
    
    # –°—á–µ—Ç—á–∏–∫ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ñ–ª—É–¥–∞ –ª–æ–≥–æ–≤ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∫–∞–¥—Ä–æ–≤
    no_frame_count = 0
    last_no_frame_log = 0
    last_stream_frame_id = -1
    
    # –°–ª–æ–≤–∞—Ä—å —ç–º–æ–¥–∑–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
    object_emojis = {
        "person": "üë§",
        "dog": "üêï",
        "cat": "üê±",
        "tv": "üì∫",
        "laptop": "üíª",
        "cell phone": "üì±",
        "chair": "ü™ë",
        "couch": "üõãÔ∏è",
        "dining table": "üçΩÔ∏è",
        "bed": "üõèÔ∏è",
        "book": "üìñ",
        "cup": "‚òï",
        "bottle": "üçº",
        "keyboard": "‚å®Ô∏è",
        "mouse": "üñ±Ô∏è",
    }
    
    # –¶–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ (BGR —Ñ–æ—Ä–º–∞—Ç –¥–ª—è OpenCV)
    # –í—ã–±—Ä–∞–Ω—ã –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è —Ö–æ—Ä–æ—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
    object_colors = {
        "person": (0, 120, 255),       # –û—Ä–∞–Ω–∂–µ–≤—ã–π (—è—Ä–∫–∏–π)
        "dog": (0, 200, 0),            # –ó–µ–ª—ë–Ω—ã–π
        "cat": (255, 100, 0),          # –°–∏–Ω–∏–π
        "tv": (255, 0, 150),           # –†–æ–∑–æ–≤–æ-—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π
        "laptop": (200, 0, 200),       # –ü—É—Ä–ø—É—Ä–Ω—ã–π
        "cell phone": (255, 150, 0),   # –ì–æ–ª—É–±–æ–π
        "chair": (0, 150, 0),          # –¢—ë–º–Ω–æ-–∑–µ–ª—ë–Ω—ã–π
        "couch": (150, 0, 150),        # –§–∏–æ–ª–µ—Ç–æ–≤—ã–π
        "dining table": (200, 150, 0), # –ë–∏—Ä—é–∑–æ–≤—ã–π
        "bed": (0, 100, 200),          # –ö–æ—Ä–∏—á–Ω–µ–≤–æ-–æ—Ä–∞–Ω–∂–µ–≤—ã–π
        "book": (0, 180, 180),         # –ñ—ë–ª—Ç–æ-–∑–µ–ª—ë–Ω—ã–π
        "cup": (50, 50, 200),          # –ö—Ä–∞—Å–Ω—ã–π
        "bottle": (150, 100, 0),       # –¢—ë–º–Ω–æ-—Å–∏–Ω–∏–π
        "keyboard": (150, 0, 100),     # –¢—ë–º–Ω–æ-—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π
        "mouse": (100, 50, 150),       # –ë–æ—Ä–¥–æ–≤—ã–π
    }

    while True:
        if stream is None:
            log("üîÑ –ö–∞–º–µ—Ä–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–±—É—é –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è...")
            time.sleep(5)
            stream = open_camera_stream()
            continue

        img, stream_frame_id, last_ok_ts = stream.get_latest()
        now = time.time()

        # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤ ‚Äî –∂–¥—ë–º. –ï—Å–ª–∏ –ø–æ—Ç–æ–∫ ‚Äú–∑–∞—Å—Ç—Ä—è–ª‚Äù (–¥–∞–≤–Ω–æ –Ω–µ –±—ã–ª–æ —É—Å–ø–µ—à–Ω—ã—Ö read) ‚Äî —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ no-frame.
        if img is None or stream_frame_id == last_stream_frame_id:
            if last_ok_ts == 0.0 or (now - last_ok_ts) > config.STREAM_STALE_SEC:
                no_frame_count += 1
                if no_frame_count == 1 or (no_frame_count % 10 == 0 and now - last_no_frame_log > 5):
                    log(f"‚ö†Ô∏è –ö–∞–¥—Ä –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è ‚Äî –ø—Ä–æ–±—É—é –µ—â—ë... (–ø–æ–ø—ã—Ç–∫–∞ {no_frame_count})")
                    last_no_frame_log = now

                if no_frame_count >= config.STREAM_RECONNECT_ATTEMPTS:
                    log(f"üîÑ –ü–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫—É –ø–æ—Å–ª–µ {no_frame_count} –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫...")
                    try:
                        stream.close()
                    except Exception:
                        pass
                    stream = None
                    time.sleep(config.STREAM_RECONNECT_DELAY)

                    for attempt in range(3):
                        stream = open_camera_stream()
                        if stream is not None:
                            no_frame_count = 0
                            last_stream_frame_id = -1
                            log("‚úÖ –ü–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")
                            break
                        log(f"‚ùå –ü–æ–ø—ã—Ç–∫–∞ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è {attempt + 1}/3 –Ω–µ —É–¥–∞–ª–∞—Å—å, –∂–¥—É...")
                        time.sleep(5 * (attempt + 1))
                    else:
                        log("‚ö†Ô∏è –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –Ω–µ —É–¥–∞–ª–∏—Å—å, –ø—Ä–æ–¥–æ–ª–∂–∞—é –ø—ã—Ç–∞—Ç—å—Å—è...")
                        no_frame_count = 0
                        time.sleep(10)
                    continue

            time.sleep(0.05)
            continue

        # –ù–æ–≤—ã–π –∫–∞–¥—Ä
        last_stream_frame_id = stream_frame_id

        # –ï—Å–ª–∏ –∫–∞–¥—Ä –ø–æ–ª—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
        if no_frame_count > 0:
            if no_frame_count > 1:
                log(f"‚úÖ –í–∏–¥–µ–æ–ø–æ—Ç–æ–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Å–ª–µ {no_frame_count} –ø–æ–ø—ã—Ç–æ–∫")
            no_frame_count = 0

        frame += 1
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        results = yolo.predict(
            img,
            imgsz=config.YOLO_IMGSZ,
            half=config.YOLO_FP16,
            verbose=False,
        )
        seen: dict[str, bool] = {}

        # ---------------- YOLO ----------------
        detected_objects = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        
        for r in results:
            boxes = r.boxes.xyxy.cpu().numpy()
            classes = r.boxes.cls.cpu().numpy().astype(int)
            confidences = r.boxes.conf.cpu().numpy()  # –ü–æ–ª—É—á–∞–µ–º confidence

            for (x1, y1, x2, y2), cls, conf in zip(boxes, classes, confidences):
                label = yolo.names.get(cls, str(cls))
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –∫–ª–∞—Å—Å—ã
                if label in config.YOLO_IGNORE_CLASSES:
                    continue
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É confidence (—Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è person –∏ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö)
                threshold = config.YOLO_PERSON_CONFIDENCE if label == "person" else config.YOLO_CONFIDENCE_THRESHOLD
                if conf < threshold:
                    continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä–µ–∫—Ç–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                emoji = object_emojis.get(label, "üì¶")
                w = int(x2 - x1)
                h = int(y2 - y1)
                detected_objects.append({
                    "label": label,
                    "emoji": emoji,
                    "confidence": float(conf),
                    "x": int(x1),
                    "y": int(y1),
                    "w": w,
                    "h": h
                })

                # –≤—Å–µ–≥–¥–∞ —Å—á–∏—Ç–∞–µ–º person –∫–∞–∫ –±–∞–∑–æ–≤—É—é —Å—É—â–Ω–æ—Å—Ç—å
                if label == "person":
                    seen["person"] = True
                else:
                    seen[label] = True

                # –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ person ‚Äî –ª–∏—Ü–∞ –Ω–µ –∏—â–µ–º
                if label != "person" or len(all_embeddings_list) == 0:
                    continue

                h, w = img.shape[:2]
                x1i, y1i = max(0, int(x1)), max(0, int(y1))
                x2i, y2i = min(w, int(x2)), min(h, int(y2))

                if x2i <= x1i or y2i <= y1i:
                    continue

                # padding –≤–æ–∫—Ä—É–≥ bounding box
                box_w = x2i - x1i
                box_h = y2i - y1i
                pad = config.FACE_PADDING_RATIO

                x1p = max(0, int(x1i - box_w * pad))
                y1p = max(0, int(y1i - box_h * pad))
                x2p = min(w, int(x2i + box_w * pad))
                y2p = min(h, int(y2i + box_h * pad))

                crop = img[y1p:y2p, x1p:x2p]
                if (
                    crop.size == 0
                    or crop.shape[0] < config.MIN_FACE_SIZE
                    or crop.shape[1] < config.MIN_FACE_SIZE
                ):
                    continue

                crop = preprocess_face_crop(crop)

                try:
                    faces = face_app.get(crop, max_num=config.MAX_FACES_PER_CROP)
                except Exception:
                    faces = []

                if not faces:
                    continue

                recognized_names: set[str] = set()

                # ----------- —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤—Å–µ –ª–∏—Ü–∞ –≤ crop -----------
                for face in faces:
                    try:
                        face_emb = np.asarray(face.embedding, dtype=np.float32)
                        if face_emb.ndim != 1:
                            continue

                        face_hash = hash(tuple(face_emb[:10].astype(int)))
                        if face_hash in face_cache:
                            cached_frame, cached_name, cached_sim = face_cache[face_hash]
                            if frame - cached_frame < cache_validity and cached_name:
                                recognized_names.add(cached_name)
                                continue

                        sims: list[tuple[float, int, str | None]] = []

                        for idx, person_embs in enumerate(all_embeddings_list):
                            confs = (
                                all_confidences_list[idx]
                                if idx < len(all_confidences_list)
                                else None
                            )
                            sim = compute_face_similarity(face_emb, person_embs, confs)
                            pname = names[idx] if idx < len(names) else None
                            sims.append((sim, idx, pname))

                        if not sims:
                            continue

                        sims.sort(reverse=True, key=lambda x: x[0])
                        best_sim, best_idx, best_name = sims[0]
                        second_sim = sims[1][0] if len(sims) > 1 else 0.0
                        diff = best_sim - second_sim

                        face_size = max(box_w, box_h)
                        thr = adaptive_threshold(face_size, config.FACE_SIM_THRESHOLD)

                        high = best_sim >= thr + 0.1
                        good_diff = diff >= config.MIN_SIM_DIFF

                        if best_sim >= thr and (good_diff or high) and best_name:
                            if high:
                                face_cache[face_hash] = (frame, best_name, best_sim)

                            recognized_names.add(best_name)
                            stats.record_person_seen(best_name)

                            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü —É–±—Ä–∞–Ω–æ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ñ–ª—É–¥–∞
                            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –ª–∏—Ü–∞—Ö –±—É–¥–µ—Ç –≤ –ª–æ–≥–∞—Ö "‚ûï –ü–æ—è–≤–∏–ª–∏—Å—å: person(–ò–º—è)"
                        else:
                            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –ª–æ–≥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                            pass

                    except Exception:
                        # –ª–æ–∫–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ ‚Äî –Ω–µ —Ä–æ–Ω—è–µ–º –ø–æ—Ç–æ–∫
                        pass

                # –µ—Å–ª–∏ –∫–æ–≥–æ-—Ç–æ —É–∑–Ω–∞–ª–∏ ‚Äî –∑–∞–º–µ–Ω—è–µ–º base "person" –Ω–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ
                if recognized_names:
                    if "person" in seen:
                        del seen["person"]
                    for nm in recognized_names:
                        seen[f"person({nm})"] = True

                    # –í–ê–ñ–ù–û: –ø–µ—Ä–µ–Ω–æ—Å–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–Ω—Ç–∏-–¥—Ä–µ–±–µ–∑–≥–∞ —Å "person" –Ω–∞ "person(–ò–º—è)",
                    # —á—Ç–æ–±—ã –∏–º—è —Å—Ä–∞–∑—É –ø–æ–ø–∞–¥–∞–ª–æ –≤ current/–ª–æ–≥ (–∞ –Ω–µ —Å–ø—É—Å—Ç—è –¥–µ—Å—è—Ç–∫–∏ –∫–∞–¥—Ä–æ–≤).
                    if "person" in tracked:
                        base_state = tracked.pop("person")
                        base_last = int(base_state.get("last", 0))
                        base_stable = int(base_state.get("stable", 1))
                        for nm in recognized_names:
                            key = f"person({nm})"
                            if key not in tracked:
                                tracked[key] = {"last": base_last, "stable": base_stable}
                            else:
                                tracked[key]["last"] = min(int(tracked[key].get("last", 0)), base_last)
                                tracked[key]["stable"] = max(int(tracked[key].get("stable", 1)), base_stable)

                # –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è —á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
                if frame % 100 == 0:
                    face_cache = {
                        k: v for k, v in face_cache.items() if frame - v[0] < cache_validity
                    }

        # ---------------- –ê–Ω—Ç–∏-–¥—Ä–µ–±–µ–∑–≥ ----------------
        # –†–∞–∑–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–∞–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (person, dog, cat) –∏ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        def get_debounce_params(label: str):
            """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (max_missing, min_stable) –¥–ª—è –æ–±—ä–µ–∫—Ç–∞."""
            base = label.split("(", 1)[0] if "(" in label else label
            if base in config.IMPORTANT_OBJECTS:
                return config.MAX_MISSING, config.MIN_STABLE
            return config.MAX_MISSING_OTHER, config.MIN_STABLE_OTHER
        
        for lbl in list(tracked.keys()):
            max_missing, min_stable = get_debounce_params(lbl)
            if lbl not in seen:
                tracked[lbl]["last"] += 1
                if tracked[lbl]["last"] > max_missing // 2:
                    tracked[lbl]["stable"] = max(0, tracked[lbl]["stable"] - 1)
                if tracked[lbl]["last"] > max_missing:
                    tracked.pop(lbl)
            else:
                tracked[lbl]["last"] = 0
                tracked[lbl]["stable"] = min(
                    tracked[lbl]["stable"] + 1, min_stable
                )

        for lbl in seen:
            if lbl not in tracked:
                tracked[lbl] = {"last": 0, "stable": 1}

        current = {
            lbl for lbl, v in tracked.items() 
            if v["stable"] >= get_debounce_params(lbl)[1]
        }

        if current != last_reported:
            added = current - last_reported
            removed = last_reported - current

            if added or removed:
                # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
                log(f"üì∏ –ö–∞–¥—Ä {frame}: –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ–±—ä–µ–∫—Ç—ã:")
                
                # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ label
                objects_by_label = {}
                for obj in detected_objects:
                    label = obj["label"]
                    # –ï—Å–ª–∏ person –±—ã–ª —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω —Å –∏–º–µ–Ω–µ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                    for seen_label in seen.keys():
                        if seen_label.startswith("person(") and label == "person":
                            label = seen_label
                            break
                    if label not in objects_by_label or obj["confidence"] > objects_by_label[label]["confidence"]:
                        objects_by_label[label] = obj
                
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
                img_with_boxes = img.copy()
                
                # –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º –≤—Å–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
                for obj in detected_objects:
                    label = obj["label"]
                    # –ï—Å–ª–∏ person –±—ã–ª —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω —Å –∏–º–µ–Ω–µ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                    display_label = label
                    for seen_label in seen.keys():
                        if seen_label.startswith("person(") and label == "person":
                            display_label = seen_label
                            break
                    
                    # –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º bounding box
                    x1, y1 = obj['x'], obj['y']
                    x2, y2 = x1 + obj['w'], y1 + obj['h']
                    
                    # –¶–≤–µ—Ç –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –ø–∞–ª–∏—Ç—Ä—ã
                    color = object_colors.get(label, (255, 0, 0))  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫—Ä–∞—Å–Ω—ã–π
                    
                    # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫ —Å –æ–±–≤–æ–¥–∫–æ–π –¥–ª—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
                    line_width = 3 if label == "person" else 2
                    # –°–Ω–∞—á–∞–ª–∞ —á—ë—Ä–Ω–∞—è –æ–±–≤–æ–¥–∫–∞ (—Ç–æ–ª—â–µ)
                    cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 0, 0), line_width + 2)
                    # –ó–∞—Ç–µ–º —Ü–≤–µ—Ç–Ω–∞—è —Ä–∞–º–∫–∞
                    cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), color, line_width)
                    
                    # –¢–µ–∫—Å—Ç —Å label –∏ confidence (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode)
                    label_text = f"{display_label} {obj['confidence']:.2f}"
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR color –≤ RGB –¥–ª—è PIL
                    bg_color_rgb = (color[2], color[1], color[0])  # BGR -> RGB
                    
                    # –†–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –∏ —Ü–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞
                    font_size = 18 if label == "person" else 16
                    text_color = (255, 255, 255)  # –ë–µ–ª—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
                    
                    # –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode
                    img_with_boxes = draw_text_unicode(
                        img_with_boxes,
                        label_text,
                        (x1, y1),
                        font_size=font_size,
                        text_color=text_color,
                        bg_color=bg_color_rgb
                    )
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ current
                for label in sorted(current):
                    if label in objects_by_label:
                        obj = objects_by_label[label]
                        log(
                            f"   {obj['emoji']} {label} "
                            f"(confidence: {obj['confidence']:.2f}) "
                            f"[x: {obj['x']}, y: {obj['y']}, w: {obj['w']}, h: {obj['h']}]"
                        )
                    else:
                        # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç–∞ –Ω–µ—Ç –≤ detected_objects (–Ω–∞–ø—Ä–∏–º–µ—Ä, person —Å –∏–º–µ–Ω–µ–º), –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–æ–¥–∑–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        emoji = object_emojis.get(label.split("(")[0] if "(" in label else label, "üì¶")
                        log(f"   {emoji} {label}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
                if config.SCREENSHOTS_ENABLED:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    screenshot_path = os.path.join(
                        config.SCREENSHOTS_DIR,
                        f"frame_{timestamp}_{frame}.jpg"
                    )
                    cv2.imwrite(screenshot_path, img_with_boxes)
                    log(f"üíæ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {screenshot_path}")

            if added:
                log(f"‚ûï –ü–æ—è–≤–∏–ª–∏—Å—å: {', '.join(sorted(added))}")
            if removed:
                log(f"‚ûñ –£—à–ª–∏: {', '.join(sorted(removed))}")

            log(
                f"üì∏ –°–µ–π—á–∞—Å –≤ –∫–∞–¥—Ä–µ: {', '.join(sorted(current)) or '–Ω–∏–∫–æ–≥–æ'}"
            )

            last_reported = current


# ============================================================
# üöÄ MAIN
# ============================================================
if __name__ == "__main__":
    log("üöÄ home-sentinel (RTSP video) —Å—Ç–∞—Ä—Ç—É–µ—Ç")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats.init_tables()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ-–¥–µ—Ç–µ–∫—Ç–æ—Ä –≤ —Ñ–æ–Ω–µ (–ø—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –ø–æ –≥—Ä–æ–º–∫–æ—Å—Ç–∏)
    # audio = AudioDetector()  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ
    # audio.start()  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è Immich
    all_embeddings_list, names, ids, all_confidences_list = load_or_refresh_cache()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –≤–∏–¥–µ–æ
    recognize_objects_and_faces(all_embeddings_list, names, all_confidences_list)
