#!/usr/bin/env python3
"""
main.py ‚Äî –º–æ–¥—É–ª—å–Ω—ã–π home-sentinel –¥–ª—è Immich.

–§—É–Ω–∫—Ü–∏–∏:
- YOLOv11 –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
- InsightFace (antelopev2) —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑ Immich)
- –ê–Ω—Ç–∏-–¥—Ä–µ–±–µ–∑–≥
- –õ–æ–≥ "–∫—Ç–æ –≤ –∫–∞–¥—Ä–µ"
- –ó–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ª—é–¥—è–º
- –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–≥–æ –∞—É–¥–∏–æ–¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Å RTSP (AudioDetector) –∏ –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∑–≤—É–∫–∞–º
"""

# –ü–µ—Ä–≤—ã–º –≥–ª—É—à–∏–º C-level stdout/stderr
import c_silence  # noqa: F401

import time
import warnings

import cv2
import numpy as np

import config
import stats
from audio_detector import AudioDetector
from camera import open_camera
from embeddings import load_or_refresh_cache
from models import init_face_analysis, init_yolo
from utils import (
    adaptive_threshold,
    ensure_dirs,
    log,
    preprocess_face_crop,
    _l2_normalize,
)

warnings.filterwarnings("ignore")


# ============================================================
# üí° –§—É–Ω–∫—Ü–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –ª–∏—Ü–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞
# ============================================================
def compute_face_similarity(
    face_emb: np.ndarray,
    person_embeddings_list: list,
    person_confidences: list | None = None,
) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ª–∏—Ü–æ–º –∏ –≤—Å–µ–º–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞.
    –ù–µ–º–Ω–æ–≥–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç confidence, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å.
    """
    if not person_embeddings_list:
        return 0.0

    face_emb = _l2_normalize(face_emb)
    max_sim = -1.0

    for idx, emb in enumerate(person_embeddings_list):
        emb = _l2_normalize(emb)
        sim = float(np.dot(face_emb, emb))

        if person_confidences and idx < len(person_confidences):
            conf = max(0.5, float(person_confidences[idx]))
            sim *= (0.7 + 0.3 * conf)

        if sim > max_sim:
            max_sim = sim

    return max_sim


# ============================================================
# üîÅ –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª: –≤–∏–¥–µ–æ, –æ–±—ä–µ–∫—Ç—ã, –ª–∏—Ü–∞, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
# ============================================================
def recognize_objects_and_faces(
    all_embeddings_list: list,
    names: list,
    all_confidences_list: list,
):
    ensure_dirs()

    cap = open_camera()
    if cap is None:
        time.sleep(5)
        cap = open_camera()
        if cap is None:
            raise RuntimeError("üö´ –ö–∞–º–µ—Ä–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

    yolo = init_yolo()
    face_app = init_face_analysis()

    tracked: dict[str, dict[str, int]] = {}
    last_reported: set[str] = set()
    frame = 0

    face_cache: dict[int, tuple[int, str | None, float]] = {}
    cache_validity = config.FACE_CACHE_VALIDITY_FRAMES

    while True:
        ret, img = cap.read()
        if not ret:
            log("‚ö†Ô∏è –ö–∞–¥—Ä –Ω–µ –ø–æ–ª—É—á–µ–Ω ‚Äî –ø—Ä–æ–±—É—é –µ—â—ë...")
            time.sleep(0.2)
            continue

        frame += 1
        results = yolo.predict(img, verbose=False)
        seen: dict[str, bool] = {}

        # ---------------- YOLO ----------------
        for r in results:
            boxes = r.boxes.xyxy.cpu().numpy()
            classes = r.boxes.cls.cpu().numpy().astype(int)

            for (x1, y1, x2, y2), cls in zip(boxes, classes):
                label = yolo.names.get(cls, str(cls))

                # –≤—Å–µ–≥–¥–∞ —Å—á–∏—Ç–∞–µ–º person –∫–∞–∫ –±–∞–∑–æ–≤—É—é —Å—É—â–Ω–æ—Å—Ç—å
                if label == "person":
                    seen["person"] = True
                else:
                    seen[label] = True

                # –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ person ‚Äî –ª–∏—Ü–∞ –Ω–µ –∏—â–µ–º
                if label != "person" or len(all_embeddings_list) == 0:
                    continue

                h, w = img.shape[:2]
                x1i, y1i = max(0, int(x1)), max(0, int(y1))
                x2i, y2i = min(w, int(x2)), min(h, int(y2))

                if x2i <= x1i or y2i <= y1i:
                    continue

                # padding –≤–æ–∫—Ä—É–≥ bounding box
                box_w = x2i - x1i
                box_h = y2i - y1i
                pad = config.FACE_PADDING_RATIO

                x1p = max(0, int(x1i - box_w * pad))
                y1p = max(0, int(y1i - box_h * pad))
                x2p = min(w, int(x2i + box_w * pad))
                y2p = min(h, int(y2i + box_h * pad))

                crop = img[y1p:y2p, x1p:x2p]
                if (
                    crop.size == 0
                    or crop.shape[0] < config.MIN_FACE_SIZE
                    or crop.shape[1] < config.MIN_FACE_SIZE
                ):
                    continue

                crop = preprocess_face_crop(crop)

                try:
                    faces = face_app.get(crop, max_num=config.MAX_FACES_PER_CROP)
                except Exception:
                    faces = []

                if not faces:
                    continue

                recognized_names: set[str] = set()

                # ----------- —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤—Å–µ –ª–∏—Ü–∞ –≤ crop -----------
                for face in faces:
                    try:
                        face_emb = np.asarray(face.embedding, dtype=np.float32)
                        if face_emb.ndim != 1:
                            continue

                        face_hash = hash(tuple(face_emb[:10].astype(int)))
                        if face_hash in face_cache:
                            cached_frame, cached_name, cached_sim = face_cache[face_hash]
                            if frame - cached_frame < cache_validity and cached_name:
                                recognized_names.add(cached_name)
                                continue

                        sims: list[tuple[float, int, str | None]] = []

                        for idx, person_embs in enumerate(all_embeddings_list):
                            confs = (
                                all_confidences_list[idx]
                                if idx < len(all_confidences_list)
                                else None
                            )
                            sim = compute_face_similarity(face_emb, person_embs, confs)
                            pname = names[idx] if idx < len(names) else None
                            sims.append((sim, idx, pname))

                        if not sims:
                            continue

                        sims.sort(reverse=True, key=lambda x: x[0])
                        best_sim, best_idx, best_name = sims[0]
                        second_sim = sims[1][0] if len(sims) > 1 else 0.0
                        diff = best_sim - second_sim

                        face_size = max(box_w, box_h)
                        thr = adaptive_threshold(face_size, config.FACE_SIM_THRESHOLD)

                        high = best_sim >= thr + 0.1
                        good_diff = diff >= config.MIN_SIM_DIFF

                        if best_sim >= thr and (good_diff or high) and best_name:
                            if high:
                                face_cache[face_hash] = (frame, best_name, best_sim)

                            recognized_names.add(best_name)
                            stats.record_person_seen(best_name)

                            log(
                                f"üîç –ö–∞–¥—Ä {frame}: {best_name} "
                                f"sim={best_sim:.3f}, diff={diff:.3f}, thr={thr:.3f}"
                            )
                        else:
                            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –ª–æ–≥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                            pass

                    except Exception:
                        # –ª–æ–∫–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ ‚Äî –Ω–µ —Ä–æ–Ω—è–µ–º –ø–æ—Ç–æ–∫
                        pass

                # –µ—Å–ª–∏ –∫–æ–≥–æ-—Ç–æ —É–∑–Ω–∞–ª–∏ ‚Äî –∑–∞–º–µ–Ω—è–µ–º base "person" –Ω–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ
                if recognized_names:
                    if "person" in seen:
                        del seen["person"]
                    for nm in recognized_names:
                        seen[f"person({nm})"] = True

                # –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è —á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
                if frame % 100 == 0:
                    face_cache = {
                        k: v for k, v in face_cache.items() if frame - v[0] < cache_validity
                    }

        # ---------------- –ê–Ω—Ç–∏-–¥—Ä–µ–±–µ–∑–≥ ----------------
        for lbl in list(tracked.keys()):
            if lbl not in seen:
                tracked[lbl]["last"] += 1
                if tracked[lbl]["last"] > config.MAX_MISSING // 2:
                    tracked[lbl]["stable"] = max(0, tracked[lbl]["stable"] - 1)
                if tracked[lbl]["last"] > config.MAX_MISSING:
                    tracked.pop(lbl)
            else:
                tracked[lbl]["last"] = 0
                tracked[lbl]["stable"] = min(
                    tracked[lbl]["stable"] + 1, config.MIN_STABLE
                )

        for lbl in seen:
            if lbl not in tracked:
                tracked[lbl] = {"last": 0, "stable": 1}

        current = {l for l, v in tracked.items() if v["stable"] >= config.MIN_STABLE}

        if current != last_reported:
            added = current - last_reported
            removed = last_reported - current

            if added:
                log(f"‚ûï –ü–æ—è–≤–∏–ª–∏—Å—å: {', '.join(sorted(added))}")
            if removed:
                log(f"‚ûñ –£—à–ª–∏: {', '.join(sorted(removed))}")

            log(
                f"üì∏ –°–µ–π—á–∞—Å –≤ –∫–∞–¥—Ä–µ: {', '.join(sorted(current)) or '–Ω–∏–∫–æ–≥–æ'}"
            )

            last_reported = current


# ============================================================
# üöÄ MAIN
# ============================================================
if __name__ == "__main__":
    log("üöÄ home-sentinel (RTSP video + RTSP audio) —Å—Ç–∞—Ä—Ç—É–µ—Ç")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats.init_tables()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ-–¥–µ—Ç–µ–∫—Ç–æ—Ä –≤ —Ñ–æ–Ω–µ (–ø—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –ø–æ –≥—Ä–æ–º–∫–æ—Å—Ç–∏)
    audio = AudioDetector()
    audio.start()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è Immich
    all_embeddings_list, names, ids, all_confidences_list = load_or_refresh_cache()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –≤–∏–¥–µ–æ
    recognize_objects_and_faces(all_embeddings_list, names, all_confidences_list)
