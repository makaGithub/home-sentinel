#!/usr/bin/env python3
"""
main.py ‚Äî –º–æ–¥—É–ª—å–Ω—ã–π home-sentinel –¥–ª—è Immich.

–§—É–Ω–∫—Ü–∏–∏:
- YOLOv11 –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
- InsightFace (antelopev2) —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑ Immich)
- –ê–Ω—Ç–∏-–¥—Ä–µ–±–µ–∑–≥
- –õ–æ–≥ "–∫—Ç–æ –≤ –∫–∞–¥—Ä–µ"
- –ó–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ª—é–¥—è–º
- –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–≥–æ –∞—É–¥–∏–æ–¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Å RTSP (AudioDetector) –∏ –∑–∞–ø–∏—Å—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∑–≤—É–∫–∞–º (–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ)
"""

# –ü–µ—Ä–≤—ã–º –≥–ª—É—à–∏–º C-level stdout/stderr
import c_silence  # noqa: F401

import os
import time
import warnings
from datetime import datetime

import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont

import config
import stats
# from audio_detector import AudioDetector  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ
from camera import open_camera
from embeddings import load_or_refresh_cache
from models import init_face_analysis, init_yolo
from utils import (
    adaptive_threshold,
    ensure_dirs,
    log,
    preprocess_face_crop,
    _l2_normalize,
)

warnings.filterwarnings("ignore")


# ============================================================
# üé® –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode
# ============================================================
def draw_text_unicode(img, text, position, font_size=20, text_color=(255, 255, 255), bg_color=None):
    """
    –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode (–≤–∫–ª—é—á–∞—è —Ä—É—Å—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã) –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ OpenCV.
    
    Args:
        img: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ OpenCV (BGR)
        text: —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
        position: (x, y) –ø–æ–∑–∏—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        font_size: —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞
        text_color: —Ü–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞ (RGB)
        bg_color: —Ü–≤–µ—Ç —Ñ–æ–Ω–∞ (RGB) –∏–ª–∏ None
    
    Returns:
        –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –æ—Ç—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
    """
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º OpenCV –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (BGR) –≤ PIL (RGB)
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —à—Ä–∏—Ñ—Ç, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º default
    try:
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π —à—Ä–∏—Ñ—Ç
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", font_size)
    except:
        try:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—É—Ç—å
            font = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf", font_size)
        except:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º default —à—Ä–∏—Ñ—Ç (–º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã)
            font = ImageFont.load_default()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–∞
    bbox = draw.textbbox((0, 0), text, font=font)
    text_width = bbox[2] - bbox[0]
    text_height = bbox[3] - bbox[1]
    
    x, y = position
    
    # –†–∏—Å—É–µ–º —Ñ–æ–Ω –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if bg_color:
        draw.rectangle(
            [(x, y - text_height - 5), (x + text_width, y + 5)],
            fill=bg_color
        )
    
    # –†–∏—Å—É–µ–º —Ç–µ–∫—Å—Ç
    draw.text((x, y - text_height), text, font=font, fill=text_color)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ OpenCV (BGR)
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)


# ============================================================
# üí° –§—É–Ω–∫—Ü–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ –ª–∏—Ü–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞
# ============================================================
def compute_face_similarity(
    face_emb: np.ndarray,
    person_embeddings_list: list,
    person_confidences: list | None = None,
) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ª–∏—Ü–æ–º –∏ –≤—Å–µ–º–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞.
    –ù–µ–º–Ω–æ–≥–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç confidence, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å.
    """
    if not person_embeddings_list:
        return 0.0

    face_emb = _l2_normalize(face_emb)
    max_sim = -1.0

    for idx, emb in enumerate(person_embeddings_list):
        emb = _l2_normalize(emb)
        sim = float(np.dot(face_emb, emb))

        if person_confidences and idx < len(person_confidences):
            conf = max(0.5, float(person_confidences[idx]))
            sim *= (0.7 + 0.3 * conf)

        if sim > max_sim:
            max_sim = sim

    return max_sim


# ============================================================
# üîÅ –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª: –≤–∏–¥–µ–æ, –æ–±—ä–µ–∫—Ç—ã, –ª–∏—Ü–∞, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
# ============================================================
def recognize_objects_and_faces(
    all_embeddings_list: list,
    names: list,
    all_confidences_list: list,
):
    ensure_dirs()

    cap = open_camera()
    if cap is None:
        time.sleep(5)
        cap = open_camera()
        if cap is None:
            raise RuntimeError("üö´ –ö–∞–º–µ—Ä–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

    yolo = init_yolo()
    face_app = init_face_analysis()

    tracked: dict[str, dict[str, int]] = {}
    last_reported: set[str] = set()
    frame = 0

    face_cache: dict[int, tuple[int, str | None, float]] = {}
    cache_validity = config.FACE_CACHE_VALIDITY_FRAMES
    
    # –°—á–µ—Ç—á–∏–∫ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ñ–ª—É–¥–∞ –ª–æ–≥–æ–≤ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∫–∞–¥—Ä–æ–≤
    no_frame_count = 0
    last_no_frame_log = 0
    
    # –°–ª–æ–≤–∞—Ä—å —ç–º–æ–¥–∑–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
    object_emojis = {
        "person": "üë§",
        "dog": "üêï",
        "cat": "üê±",
        "tv": "üì∫",
        "laptop": "üíª",
        "cell phone": "üì±",
        "chair": "ü™ë",
        "couch": "üõãÔ∏è",
        "dining table": "üçΩÔ∏è",
        "bed": "üõèÔ∏è",
        "book": "üìñ",
        "cup": "‚òï",
        "bottle": "üçº",
        "keyboard": "‚å®Ô∏è",
        "mouse": "üñ±Ô∏è",
    }
    
    # –¶–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ (BGR —Ñ–æ—Ä–º–∞—Ç –¥–ª—è OpenCV)
    object_colors = {
        "person": (255, 100, 0),      # –°–∏–Ω–∏–π
        "dog": (0, 255, 255),          # –ñ–µ–ª—Ç—ã–π
        "cat": (255, 165, 0),          # –û—Ä–∞–Ω–∂–µ–≤—ã–π
        "tv": (255, 0, 255),           # –ü—É—Ä–ø—É—Ä–Ω—ã–π
        "laptop": (255, 20, 147),      # –†–æ–∑–æ–≤—ã–π
        "cell phone": (0, 191, 255),   # –ì–æ–ª—É–±–æ–π
        "chair": (34, 139, 34),        # –ó–µ–ª–µ–Ω—ã–π
        "couch": (139, 0, 139),        # –¢–µ–º–Ω–æ-—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π
        "dining table": (0, 206, 209), # –ë–∏—Ä—é–∑–æ–≤—ã–π
        "bed": (255, 140, 0),          # –¢–µ–º–Ω–æ-–æ—Ä–∞–Ω–∂–µ–≤—ã–π
        "book": (255, 215, 0),         # –ó–æ–ª–æ—Ç–æ–π
        "cup": (255, 69, 0),           # –ö—Ä–∞—Å–Ω–æ-–æ—Ä–∞–Ω–∂–µ–≤—ã–π
        "bottle": (0, 250, 154),       # –ó–µ–ª–µ–Ω–æ-–≥–æ–ª—É–±–æ–π
        "keyboard": (138, 43, 226),    # –§–∏–æ–ª–µ—Ç–æ–≤—ã–π
        "mouse": (255, 105, 180),      # –†–æ–∑–æ–≤–æ-–∫—Ä–∞—Å–Ω—ã–π
    }

    while True:
        ret, img = cap.read()
        if not ret:
            no_frame_count += 1
            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 10 –ø–æ–ø—ã—Ç–æ–∫ –∏–ª–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–µ
            if no_frame_count == 1 or (no_frame_count % 10 == 0 and time.time() - last_no_frame_log > 5):
                log(f"‚ö†Ô∏è –ö–∞–¥—Ä –Ω–µ –ø–æ–ª—É—á–µ–Ω ‚Äî –ø—Ä–æ–±—É—é –µ—â—ë... (–ø–æ–ø—ã—Ç–∫–∞ {no_frame_count})")
                last_no_frame_log = time.time()
            time.sleep(0.2)
            continue

        # –ï—Å–ª–∏ –∫–∞–¥—Ä –ø–æ–ª—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
        if no_frame_count > 0:
            if no_frame_count > 1:
                log(f"‚úÖ –í–∏–¥–µ–æ–ø–æ—Ç–æ–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Å–ª–µ {no_frame_count} –ø–æ–ø—ã—Ç–æ–∫")
            no_frame_count = 0

        frame += 1
        results = yolo.predict(img, verbose=False)
        seen: dict[str, bool] = {}

        # ---------------- YOLO ----------------
        detected_objects = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        
        for r in results:
            boxes = r.boxes.xyxy.cpu().numpy()
            classes = r.boxes.cls.cpu().numpy().astype(int)
            confidences = r.boxes.conf.cpu().numpy()  # –ü–æ–ª—É—á–∞–µ–º confidence

            for (x1, y1, x2, y2), cls, conf in zip(boxes, classes, confidences):
                label = yolo.names.get(cls, str(cls))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä–µ–∫—Ç–µ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                emoji = object_emojis.get(label, "üì¶")
                w = int(x2 - x1)
                h = int(y2 - y1)
                detected_objects.append({
                    "label": label,
                    "emoji": emoji,
                    "confidence": float(conf),
                    "x": int(x1),
                    "y": int(y1),
                    "w": w,
                    "h": h
                })

                # –≤—Å–µ–≥–¥–∞ —Å—á–∏—Ç–∞–µ–º person –∫–∞–∫ –±–∞–∑–æ–≤—É—é —Å—É—â–Ω–æ—Å—Ç—å
                if label == "person":
                    seen["person"] = True
                else:
                    seen[label] = True

                # –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ person ‚Äî –ª–∏—Ü–∞ –Ω–µ –∏—â–µ–º
                if label != "person" or len(all_embeddings_list) == 0:
                    continue

                h, w = img.shape[:2]
                x1i, y1i = max(0, int(x1)), max(0, int(y1))
                x2i, y2i = min(w, int(x2)), min(h, int(y2))

                if x2i <= x1i or y2i <= y1i:
                    continue

                # padding –≤–æ–∫—Ä—É–≥ bounding box
                box_w = x2i - x1i
                box_h = y2i - y1i
                pad = config.FACE_PADDING_RATIO

                x1p = max(0, int(x1i - box_w * pad))
                y1p = max(0, int(y1i - box_h * pad))
                x2p = min(w, int(x2i + box_w * pad))
                y2p = min(h, int(y2i + box_h * pad))

                crop = img[y1p:y2p, x1p:x2p]
                if (
                    crop.size == 0
                    or crop.shape[0] < config.MIN_FACE_SIZE
                    or crop.shape[1] < config.MIN_FACE_SIZE
                ):
                    continue

                crop = preprocess_face_crop(crop)

                try:
                    faces = face_app.get(crop, max_num=config.MAX_FACES_PER_CROP)
                except Exception:
                    faces = []

                if not faces:
                    continue

                recognized_names: set[str] = set()

                # ----------- —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤—Å–µ –ª–∏—Ü–∞ –≤ crop -----------
                for face in faces:
                    try:
                        face_emb = np.asarray(face.embedding, dtype=np.float32)
                        if face_emb.ndim != 1:
                            continue

                        face_hash = hash(tuple(face_emb[:10].astype(int)))
                        if face_hash in face_cache:
                            cached_frame, cached_name, cached_sim = face_cache[face_hash]
                            if frame - cached_frame < cache_validity and cached_name:
                                recognized_names.add(cached_name)
                                continue

                        sims: list[tuple[float, int, str | None]] = []

                        for idx, person_embs in enumerate(all_embeddings_list):
                            confs = (
                                all_confidences_list[idx]
                                if idx < len(all_confidences_list)
                                else None
                            )
                            sim = compute_face_similarity(face_emb, person_embs, confs)
                            pname = names[idx] if idx < len(names) else None
                            sims.append((sim, idx, pname))

                        if not sims:
                            continue

                        sims.sort(reverse=True, key=lambda x: x[0])
                        best_sim, best_idx, best_name = sims[0]
                        second_sim = sims[1][0] if len(sims) > 1 else 0.0
                        diff = best_sim - second_sim

                        face_size = max(box_w, box_h)
                        thr = adaptive_threshold(face_size, config.FACE_SIM_THRESHOLD)

                        high = best_sim >= thr + 0.1
                        good_diff = diff >= config.MIN_SIM_DIFF

                        if best_sim >= thr and (good_diff or high) and best_name:
                            if high:
                                face_cache[face_hash] = (frame, best_name, best_sim)

                            recognized_names.add(best_name)
                            stats.record_person_seen(best_name)

                            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü —É–±—Ä–∞–Ω–æ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ñ–ª—É–¥–∞
                            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –ª–∏—Ü–∞—Ö –±—É–¥–µ—Ç –≤ –ª–æ–≥–∞—Ö "‚ûï –ü–æ—è–≤–∏–ª–∏—Å—å: person(–ò–º—è)"
                        else:
                            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –ª–æ–≥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                            pass

                    except Exception:
                        # –ª–æ–∫–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ ‚Äî –Ω–µ —Ä–æ–Ω—è–µ–º –ø–æ—Ç–æ–∫
                        pass

                # –µ—Å–ª–∏ –∫–æ–≥–æ-—Ç–æ —É–∑–Ω–∞–ª–∏ ‚Äî –∑–∞–º–µ–Ω—è–µ–º base "person" –Ω–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ
                if recognized_names:
                    if "person" in seen:
                        del seen["person"]
                    for nm in recognized_names:
                        seen[f"person({nm})"] = True

                # –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è —á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
                if frame % 100 == 0:
                    face_cache = {
                        k: v for k, v in face_cache.items() if frame - v[0] < cache_validity
                    }

        # ---------------- –ê–Ω—Ç–∏-–¥—Ä–µ–±–µ–∑–≥ ----------------
        for lbl in list(tracked.keys()):
            if lbl not in seen:
                tracked[lbl]["last"] += 1
                if tracked[lbl]["last"] > config.MAX_MISSING // 2:
                    tracked[lbl]["stable"] = max(0, tracked[lbl]["stable"] - 1)
                if tracked[lbl]["last"] > config.MAX_MISSING:
                    tracked.pop(lbl)
            else:
                tracked[lbl]["last"] = 0
                tracked[lbl]["stable"] = min(
                    tracked[lbl]["stable"] + 1, config.MIN_STABLE
                )

        for lbl in seen:
            if lbl not in tracked:
                tracked[lbl] = {"last": 0, "stable": 1}

        current = {l for l, v in tracked.items() if v["stable"] >= config.MIN_STABLE}

        if current != last_reported:
            added = current - last_reported
            removed = last_reported - current

            if added or removed:
                # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
                log(f"üì∏ –ö–∞–¥—Ä {frame}: –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ–±—ä–µ–∫—Ç—ã:")
                
                # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ label
                objects_by_label = {}
                for obj in detected_objects:
                    label = obj["label"]
                    # –ï—Å–ª–∏ person –±—ã–ª —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω —Å –∏–º–µ–Ω–µ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                    for seen_label in seen.keys():
                        if seen_label.startswith("person(") and label == "person":
                            label = seen_label
                            break
                    if label not in objects_by_label or obj["confidence"] > objects_by_label[label]["confidence"]:
                        objects_by_label[label] = obj
                
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏
                img_with_boxes = img.copy()
                
                # –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º –≤—Å–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
                for obj in detected_objects:
                    label = obj["label"]
                    # –ï—Å–ª–∏ person –±—ã–ª —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω —Å –∏–º–µ–Ω–µ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                    display_label = label
                    for seen_label in seen.keys():
                        if seen_label.startswith("person(") and label == "person":
                            display_label = seen_label
                            break
                    
                    # –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º bounding box
                    x1, y1 = obj['x'], obj['y']
                    x2, y2 = x1 + obj['w'], y1 + obj['h']
                    
                    # –¶–≤–µ—Ç –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –ø–∞–ª–∏—Ç—Ä—ã
                    color = object_colors.get(label, (255, 0, 0))  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫—Ä–∞—Å–Ω—ã–π
                    
                    # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫ —Å –±–æ–ª–µ–µ —Ç–æ–ª—Å—Ç–æ–π –ª–∏–Ω–∏–µ–π –¥–ª—è person
                    line_width = 3 if label == "person" else 2
                    cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), color, line_width)
                    
                    # –¢–µ–∫—Å—Ç —Å label –∏ confidence (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode)
                    label_text = f"{display_label} {obj['confidence']:.2f}"
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR color –≤ RGB –¥–ª—è PIL
                    bg_color_rgb = (color[2], color[1], color[0])  # BGR -> RGB
                    
                    # –†–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –∏ —Ü–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞
                    font_size = 18 if label == "person" else 16
                    text_color = (255, 255, 255)  # –ë–µ–ª—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
                    
                    # –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode
                    img_with_boxes = draw_text_unicode(
                        img_with_boxes,
                        label_text,
                        (x1, y1),
                        font_size=font_size,
                        text_color=text_color,
                        bg_color=bg_color_rgb
                    )
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ current
                for label in sorted(current):
                    if label in objects_by_label:
                        obj = objects_by_label[label]
                        log(
                            f"   {obj['emoji']} {label} "
                            f"(confidence: {obj['confidence']:.2f}) "
                            f"[x: {obj['x']}, y: {obj['y']}, w: {obj['w']}, h: {obj['h']}]"
                        )
                    else:
                        # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç–∞ –Ω–µ—Ç –≤ detected_objects (–Ω–∞–ø—Ä–∏–º–µ—Ä, person —Å –∏–º–µ–Ω–µ–º), –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–æ–¥–∑–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        emoji = object_emojis.get(label.split("(")[0] if "(" in label else label, "üì¶")
                        log(f"   {emoji} {label}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                screenshot_path = os.path.join(
                    config.SCREENSHOTS_DIR,
                    f"frame_{frame}_{timestamp}.jpg"
                )
                cv2.imwrite(screenshot_path, img_with_boxes)
                log(f"üíæ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {screenshot_path}")

            if added:
                log(f"‚ûï –ü–æ—è–≤–∏–ª–∏—Å—å: {', '.join(sorted(added))}")
            if removed:
                log(f"‚ûñ –£—à–ª–∏: {', '.join(sorted(removed))}")

            log(
                f"üì∏ –°–µ–π—á–∞—Å –≤ –∫–∞–¥—Ä–µ: {', '.join(sorted(current)) or '–Ω–∏–∫–æ–≥–æ'}"
            )

            last_reported = current


# ============================================================
# üöÄ MAIN
# ============================================================
if __name__ == "__main__":
    log("üöÄ home-sentinel (RTSP video) —Å—Ç–∞—Ä—Ç—É–µ—Ç")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats.init_tables()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ-–¥–µ—Ç–µ–∫—Ç–æ—Ä –≤ —Ñ–æ–Ω–µ (–ø—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –ø–æ –≥—Ä–æ–º–∫–æ—Å—Ç–∏)
    # audio = AudioDetector()  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ
    # audio.start()  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è Immich
    all_embeddings_list, names, ids, all_confidences_list = load_or_refresh_cache()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –≤–∏–¥–µ–æ
    recognize_objects_and_faces(all_embeddings_list, names, all_confidences_list)
